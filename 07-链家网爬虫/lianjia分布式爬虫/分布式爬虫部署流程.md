### 分布式爬虫：###

#### redis配置：####
1. 安装一个ubuntu系统虚拟机作为redis服务器，在ubuntu上安装redis：sudo apt install redis-server
2. 连接reids服务器：redis-cli -h [ip地址] -p [端口号]
3. 在其他电脑上连接本机的redis服务器：在/etc/redis/redis.conf中，修改bind，把redis服务器的ip地址加进去。示例如下：

    ```shell
    bind 192.168.175.129 127.0.0.1
    或
    bind 0.0.0.0
    ```

### 爬虫部署：###
1. 在服务器上安装scrapyd：`pip3 install scrapyd`。
2. 从`/usr/local/lib/python3.5/dist-packages/scrapyd`下拷贝出`default_scrapyd.conf`放到`/etc/scrapyd/scrapyd.conf`。
3. 修改`/etc/scrapyd/scrapyd.conf`中的`bind_address`为自己的IP地址。
4. 重新安装`twisted`：

    ```
    pip uninstall twisted
    pip install twisted==18.9.0
    ```
    如果这一步不做，后期会出现intxxx的错误。
5. 在redis服务器创建目录`/srv/lianjia`，存放爬虫信息。
6. 在开发机（window电脑上）安装`pip install scrapyd-client`。修改`python/Script/scrapyd-deploy`为`scrapyd-deploy.py`；macos上安装`pip install scrapyd-client`即可。
7. 在项目中，找到`scrapy.cfg`，然后配置如下：

    ```python
    [settings]
    default = lianjia.settings

    [deploy]
    # 下面这个url要取消注释
    url = http://服务器的IP地址:6800/
    project = lianjia
    ```
8. 在项目所在的路径执行命令生成版本号并上传爬虫代码：`scrapyd-deploy`。如果一次性想要把代码上传到多个服务器，那么可以修改`scrapy.cfg`为如下：

    ```python
    [settings]
    default = lianjia.settings

    [deploy:服务器1]
    # 下面这个url要取消注释
    url = http://服务器1的IP地址:6800/
    project = lianjia

    [deploy:服务器2]
    # 下面这个url要取消注释
    url = http://服务器2的IP地址:6800/
    project = lianjia
    ```
    在redis服务器执行`sudo scrapyd`，在本机使用`scrapyd-deploy -a`就可以全部上传了。
 
9. curl for windows下载地址：`https://curl.haxx.se/windows/`，解压后双击打开bin/curl.exe即可在cmd中使用了。mac自带curl命令。
10. 在cmd中使用命令运行爬虫：

    ```
 	1、运行爬虫
	curl http://192.168.74.149:6800/schedule.json -d project=lianjia -d spider=lianjia_spider

	2、关闭爬虫
	curl http://192.168.74.149:6800/cancel.json -d project=lianjia -d job=ad379a82bab711e9a1ce000c29f6d2b3
    ```
11. 可以通过浏览器http://192.168.74.149:6800/,查看scrapyd的运行情况。
12. 如果后期修改了爬虫的代码，那么需要重新部署，然后服务器的scrapyd服务重新启动一下。
13. 更多的API介绍：https://scrapyd.readthedocs.io/en/stable/api.html

### 分布式爬虫部署：###
1. 克隆一个ubuntu系统，作为爬虫服务器。目前现有一个redis服务器（ubuntu），两个爬虫服务器（macos和ubuntu)。
2. 在redis服务器启动redis服务。
3. 在redis服务器创建目录`/srv/lianjia`，存放爬虫信息。
4. 将爬虫的类从`scrapy.Spider`变成`scrapy_redis.spiders.RedisSpider`;或者是从`scrapy.CrawlSpider`变成`scrapy_redis.spiders.RedisCrawlSpider`。
5. 将爬虫中的`start_urls`注释掉。增加一个`redis_key="xxx"`。这个redis_key是为了以后在redis中控制爬虫启动的。 爬虫的第一个url，就是在redis中通过这个发送出去的。
6. 配置修改(settings文件):

	```config
	# 确保request存储到redis中
	SCHEDULER = "scrapy_redis.scheduler.Scheduler"
	# 确保所有爬虫共享相同的去重指纹
	DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
	# 设置redis为item pipeline
	ITEM_PIPELINES = {
	      'scrapy_redis.pipelines.RedisPipeline': 300
	}
	# 在redis中保持scrapy-redis用到的队列，不会清理redis中的队列，从而可以实现暂停和恢复的功能。
	SCHEDULER_PERSIST = True
	# 设置连接redis信息
	REDIS_HOST = '192.168.74.149'
	REDIS_PORT = 6379
	```
	
7. 配置修改（scrapy.cfg）

	```
	[settings]
	default = lianjia.settings

	[deploy:lj1]
	url = http://192.168.74.149:6800/
	project = lianjia

	# 代码上传多个爬虫服务器
	[deploy:lj2]
	url = http://192.168.74.150:6800/
	project = lianjia
	```

8. 然后在本机使用`scrapyd-deploy -a`就可以全部上传了。
9. 可以通过浏览器`http://192.168.74.150:6800/`,查看爬虫服务器scrapyd的运行情况。
10. 运行爬虫:
	
	```runspider
	1. 在爬虫服务器上。进入爬虫文件所在的路径，然后输入命令:scrapy runspider [爬虫名字]。 
	2. 在Redis服务器上，推入一个开始的url链接:redis-cli>lpush[redis_key]start_url开始爬取。
	```
11. 在cmd中使用命令运行爬虫：

    ```
 	1、运行爬虫
	curl http://192.168.74.150:6800/schedule.json -d project=lianjia -d spider=lianjia_spider
	2、关闭爬虫
	curl http://192.168.74.149:6800/cancel.json -d project=lianjia -d job=ad379a82bab711e9a1ce000c29f6d2b3
	3、连接reids服务器：redis-cli -h [ip地址] -p [端口号],在Redis服务器上，推入一个开始的url链接: redis-cli>lpush [redis_key] start_url开始爬取。
    ```